{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "FRENCH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_from_prompt(prompt):\n",
    "    messages = []\n",
    "    with open(f\"prompts/{prompt}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        role = line.split(\" \")[0]\n",
    "        content = line[len(role)+1:]\n",
    "        messages.append({\"role\": role, \"content\": content})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 13/27 [00:55<01:02,  4.45s/it]"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display:\u001b[38;5;28mprint\u001b[39m(response, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     48\u001b[0m new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_per_char\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbefore_stream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_init\u001b[39m\u001b[38;5;124m\"\u001b[39m: before_stream \u001b[38;5;241m-\u001b[39m start,\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_total\u001b[39m\u001b[38;5;124m\"\u001b[39m: end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     54\u001b[0m }])\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(time_df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     56\u001b[0m     time_df \u001b[38;5;241m=\u001b[39m new_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# for all prompts evaluate the model\n",
    "\n",
    "# all the text files in the prompts directory\n",
    "prompts = [f for f in os.listdir(\"prompts\") if f.endswith(\".txt\")]\n",
    "# models = [\"gemma\", \"llama2\", \"mistral\", \"openbuddy-mistral\", \"openchat\", \"openhermes\", \"qwen:4b\", \"qwen:7b\", \"vigogne\", \"vigostral\"]\n",
    "models = [\"openbuddy-mistral\", \"vigogne\", \"vigostral\"]\n",
    "\n",
    "display = False\n",
    "display_minimal = True\n",
    "\n",
    "for round_number in range(500):\n",
    "\n",
    "    if display or display_minimal:print(\"Starting round\", round_number)\n",
    "\n",
    "    time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    time_df = pd.DataFrame(columns=[\"model\", \"prompt\", \"time_per_char\", \"time_init\", \"time_total\"])\n",
    "\n",
    "    # new tqdm progress bar\n",
    "    pbar = tqdm.tqdm(total=len(models) * len(prompts))\n",
    "\n",
    "    for model in models:\n",
    "        if display:print(f\"  Model: {model}\")\n",
    "        ollama.chat(\n",
    "            model=model,\n",
    "            messages=[],\n",
    "            stream=False\n",
    "        )\n",
    "        for prompt in prompts:\n",
    "            if display:print(f\"Prompt: {prompt}\")\n",
    "            messages = messages_from_prompt(prompt)\n",
    "\n",
    "            response = \"\"\n",
    "            first_chunk = True\n",
    "            start = time.time()\n",
    "            stream = ollama.chat(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                stream=True\n",
    "            )\n",
    "            for chunk in stream:\n",
    "                if first_chunk:\n",
    "                    before_stream = time.time()\n",
    "                    first_chunk = False\n",
    "                response += chunk['message']['content']\n",
    "                if display:print(response, end=\"\\r\")\n",
    "            end = time.time()\n",
    "\n",
    "            new_df = pd.DataFrame([{\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"time_per_char\": (end - before_stream) / len(response),\n",
    "                \"time_init\": before_stream - start,\n",
    "                \"time_total\": end - start\n",
    "            }])\n",
    "            if len(time_df) == 0:\n",
    "                time_df = new_df.copy()\n",
    "            else:\n",
    "                time_df = pd.concat([time_df, new_df])\n",
    "            if display:print(f\"Time: {end - start}\")\n",
    "            model_str = model.replace(\":\", \"\")\n",
    "            with open(f\"responses/{model_str}_chat_{time_str}_{prompt}\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response)\n",
    "            \n",
    "            # update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    # write the time df to a file\n",
    "    with open(f\"time_dfs/time_df_{time_str}.csv\", \"w\") as f:\n",
    "        time_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
