{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import ollama\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to video reference: https://www.youtube.com/watch?v=V1Mz8gMBDMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    with open(filename, encoding='utf-8-sig') as f:\n",
    "        paragraphs = []\n",
    "        buffer = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                buffer.append(line)\n",
    "            elif len(buffer):\n",
    "                paragraphs.append(\" \".join(buffer))\n",
    "                buffer = []\n",
    "        if len(buffer):\n",
    "            paragraphs.append(\" \".join(buffer))\n",
    "    return paragraphs\n",
    "\n",
    "def save_embeddings(filename, embeddings):\n",
    "    # create directory if not exists\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.makedirs(\"embeddings\")\n",
    "    # dump embedding to json file\n",
    "    with open(f\"embeddings/{filename}.json\", 'w') as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    # check if file exists\n",
    "    if not os.path.exists(f\"embeddings/{filename}.json\"):\n",
    "        return False\n",
    "    with open(f\"embeddings/{filename}.json\", 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_embeddings(filename, modelname, chunks):\n",
    "    # check if embeddings are already computed\n",
    "    if filename and ((embeddings := load_embeddings(filename)) is not False):\n",
    "        return embeddings\n",
    "    # compute embeddings\n",
    "    embeddings = [\n",
    "            ollama.embeddings(model=modelname, prompt=chunk)[\"embedding\"] \n",
    "            for chunk in chunks\n",
    "        ]\n",
    "    # save embeddings\n",
    "    if filename:\n",
    "        save_embeddings(filename, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def find_most_similar(needle, haystack, sort=True):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item))\n",
    "        for item in haystack\n",
    "    ]\n",
    "    if not sort:\n",
    "        return zip(similarity_scores, range(len(haystack)))\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)\n",
    "\n",
    "def find_most_similar_from_multiple(needles, haystacks, sort=True):\n",
    "    total_score_chunks = []\n",
    "    for query_embedding in needles:\n",
    "        chunks = find_most_similar(query_embedding, haystacks, sort=False)\n",
    "        total_score_chunks.append(chunks)\n",
    "    len_query = len(needles)\n",
    "    # addition of the scores of the chunks\n",
    "    total_score = [0] * len(haystacks)\n",
    "    for chunks in total_score_chunks:\n",
    "        for score, index in chunks:\n",
    "            total_score[index] += score / len_query\n",
    "    if not sort:\n",
    "        return zip(total_score, range(len(haystacks)))\n",
    "    return sorted(zip(total_score, range(len(haystacks))), reverse=True)\n",
    "\n",
    "def parse_response_llm(response):\n",
    "    result = []\n",
    "    separators = [\". \", \": \", \") \", \".\", \":\", \")\"]\n",
    "    for line_raw in response.split(\"\\n\"):\n",
    "        line = line_raw.strip()\n",
    "        if line:\n",
    "            for sep in separators:\n",
    "                if sep in line:\n",
    "                    result.append(line.split(sep)[1])\n",
    "                    break\n",
    "    return result\n",
    "\n",
    "def get_similar_chunks(chunks, threshold=0.62):\n",
    "    return [(score, index) for score, index in chunks if score >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Est-ce que tu pourrais m'en dire plus sur cette entreprise ?\"\n",
    "queries = [prompt]\n",
    "modelname = \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_embeddings = get_embeddings(None, modelname, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est-ce que tu pourrais m'en dire plus sur cette entreprise ?\n",
      "Pourraient-tu me donner des informations supplémentaires sur cette entreprise ?\n",
      "Existe-t-il de plus amples détails sur ce qui concernent cette entreprise que tu pourrais m'en partager ?\n",
      "Serait-il possible de fournir des renseignements complémentaires sur l'entreprise en question ?\n",
      "Pouvez-vous me donner plus de connaissances détaillées sur cet entrepreneur ou cette organisation ?\n",
      "Aimerais-je recevoir plus d'informations contextuelles sur cette entreprise ?\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Vous êtes un assistant de modèle linguistique d'IA. Votre tâche consiste à générer cinq versions françaises différentes de la question donnée par l'utilisateur afin d'extraire les documents pertinents d'une base de données vectorielle. \n",
    "En générant des perspectives multiples sur la question de l'utilisateur, votre objectif est d'aider l'utilisateur à surmonter certaines limites de la recherche de similarité basée sur la distance.\n",
    "l'utilisateur à surmonter certaines des limites de la recherche de similarité basée sur la distance. \n",
    "Fournissez ces questions alternatives séparées par des nouvelles lignes.\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=modelname,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Question originale : \" + prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "queries = [prompt] + parse_response_llm(response[\"message\"][\"content\"])\n",
    "\n",
    "print(\"\\n\".join(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain the potential interesting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_filenames = [\"pacte_novation_2_short.txt\", \"jeux_olympiques_paris_short.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_paragraphs length: 1\n",
      "Embeddings for pacte_novation_2_short.txt computed, length: 1\n",
      "current_paragraphs length: 1\n",
      "Embeddings for jeux_olympiques_paris_short.txt computed, length: 2\n"
     ]
    }
   ],
   "source": [
    "# creating embeddings for each description (entire text)\n",
    "embeddings = []\n",
    "paragraphs = []\n",
    "for filename in description_filenames:\n",
    "    current_paragraphs = [\" \".join(parse_file(f\"{filename}\"))]\n",
    "    print(f\"current_paragraphs length: {len(current_paragraphs)}\")\n",
    "    embeddings += get_embeddings(filename, modelname, current_paragraphs)\n",
    "    print(f\"Embeddings for {filename} computed, length: {len(embeddings)}\")\n",
    "    paragraphs += current_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query embeddings: 1\n",
      "paragraphs: 2\n",
      "embeddings: 2\n",
      "most similar chunks: [(0.4882849055500085, 0), (0.4861526996652893, 1)]\n",
      "Score: 0.4882849055500085\n",
      "Description: pacte_novation_2_short.txt\n",
      "\n",
      "Score: 0.4861526996652893\n",
      "Description: jeux_olympiques_paris_short.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_similar_chunks = find_most_similar_from_multiple(queries_embeddings, embeddings)\n",
    "\n",
    "# print all shapes\n",
    "print(\"query embeddings:\", len(queries_embeddings))\n",
    "print(\"paragraphs:\", len(paragraphs))\n",
    "print(\"embeddings:\", len(embeddings))\n",
    "\n",
    "\n",
    "print(\"most similar chunks:\", most_similar_chunks)\n",
    "\n",
    "for score, index in most_similar_chunks:\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Description: {description_filenames[index]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_chunks = get_similar_chunks(most_similar_chunks, threshold=0.5)\n",
    "\n",
    "for score, index in similar_chunks:\n",
    "    print(f\"Score: {score:.2f}\")\n",
    "    print(paragraphs[index])\n",
    "    print()\n",
    "\n",
    "similar_filenames = [description_filenames[index] for _, index in similar_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract the relevant information from the previous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "filenames = [filename.replace(\"_short\", \"\") for filename in similar_filenames]\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "paragraphs = []\n",
    "\n",
    "for filename in filenames:\n",
    "    paragraphs += parse_file(filename)\n",
    "    embeddings += get_embeddings(filename, modelname, paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the cosine similarity between the queries and the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the chunks by the total score\n",
    "sorted_chunks = find_most_similar_from_multiple(queries_embeddings, embeddings, sort=True)\n",
    "\n",
    "for score, index in sorted_chunks:\n",
    "    print(f\"Score: {score:.2f}\")\n",
    "    print(paragraphs[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when score >= 0.65, we consider the document as relevant\n",
    "similar_chunks = get_similar_chunks(sorted_chunks)\n",
    "similar_chunks = similar_chunks[:5]\n",
    "\n",
    "for score, index in similar_chunks:\n",
    "    print(f\"Score: {score:.2f}\")\n",
    "    print(paragraphs[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
